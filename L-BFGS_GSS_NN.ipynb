{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e95919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from NN_arch import LSTM_sMNIST, LeNet, FCP, Autoencoder\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 50, bias=False)\n",
    "        self.fc2 = nn.Linear(50, 50, bias=False)\n",
    "        self.fc3 = nn.Linear(50, 10, bias=False)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.softplus(self.fc1(x))\n",
    "        x = F.softplus(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "class LBFGS_Brent(Optimizer):\n",
    "    \"\"\"L-BFGS optimizer with Brent's method for line search.\n",
    "    \n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize\n",
    "        lr (float, optional): learning rate (not used, kept for compatibility)\n",
    "        max_history (int, optional): maximum number of correction pairs to store (default: 10)\n",
    "        line_search_budget (int, optional): maximum function evaluations for line search (default: 6)\n",
    "        tolerance (float, optional): convergence tolerance for line search (default: 1e-4)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params, lr=1.0, max_history=10, line_search_budget=6, tolerance=1e-4):\n",
    "        defaults = dict(lr=lr, max_history=max_history, \n",
    "                       line_search_budget=line_search_budget, \n",
    "                       tolerance=tolerance)\n",
    "        super(LBFGS_Brent, self).__init__(params, defaults)\n",
    "        \n",
    "        # Initialize storage for s and y vectors\n",
    "        self.state['s_list'] = []\n",
    "        self.state['y_list'] = []\n",
    "        self.state['prev_grad'] = None\n",
    "        self.state['initialized'] = False\n",
    "        \n",
    "    def _gather_flat_params(self):\n",
    "        \"\"\"Gather all parameters into a single flat tensor.\"\"\"\n",
    "        views = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    view = p.data.new(p.data.numel()).zero_()\n",
    "                else:\n",
    "                    view = p.data.view(-1)\n",
    "                views.append(view)\n",
    "        return torch.cat(views, 0)\n",
    "    \n",
    "    def _gather_flat_grad(self):\n",
    "        \"\"\"Gather all gradients into a single flat tensor.\"\"\"\n",
    "        views = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    view = p.data.new(p.data.numel()).zero_()\n",
    "                else:\n",
    "                    view = p.grad.data.view(-1)\n",
    "                views.append(view)\n",
    "        return torch.cat(views, 0)\n",
    "    \n",
    "    def _set_flat_params(self, flat_params):\n",
    "        \"\"\"Set all parameters from a single flat tensor.\"\"\"\n",
    "        offset = 0\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                numel = p.numel()\n",
    "                p.data.copy_(flat_params[offset:offset + numel].view_as(p.data))\n",
    "                offset += numel\n",
    "    \n",
    "    def _clone_params(self):\n",
    "        \"\"\"Clone current parameters.\"\"\"\n",
    "        return [p.clone(memory_format=torch.contiguous_format) for group in self.param_groups for p in group['params']]\n",
    "    \n",
    "    def _set_params(self, params_data):\n",
    "        \"\"\"Set parameters from a list of tensors.\"\"\"\n",
    "        for p, pdata in zip([p for group in self.param_groups for p in group['params']], params_data):\n",
    "            p.copy_(pdata)\n",
    "    \n",
    "    def _brent_line_search(self, closure, weights, grad, cur_loss, budget):\n",
    "        \"\"\"Brent's method for line search.\n",
    "        \n",
    "        Args:\n",
    "            closure: A closure that reevaluates the model and returns the loss\n",
    "            weights: Current flat parameter vector\n",
    "            grad: Search direction (note: we minimize in direction -grad)\n",
    "            cur_loss: Current loss value\n",
    "            budget: Maximum number of function evaluations\n",
    "            \n",
    "        Returns:\n",
    "            step_size: Optimal step size\n",
    "            new_loss: Loss at optimal step size\n",
    "        \"\"\"\n",
    "        tolerance = self.param_groups[0]['tolerance']\n",
    "        count_eval = 0\n",
    "        k = 0\n",
    "        a, u, b = 0, None, None\n",
    "        f_a, f_u, f_b = cur_loss, None, None\n",
    "        \n",
    "        mag_grad = torch.norm(grad, p=2).item()\n",
    "        normalized_grad = grad / mag_grad\n",
    "        \n",
    "        # Initial step\n",
    "        new_step = 0.2 * 2**k * mag_grad\n",
    "        new_weights = weights - new_step * normalized_grad\n",
    "        self._set_flat_params(new_weights)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            new_loss = closure()\n",
    "        count_eval += 1\n",
    "        \n",
    "        # Bounding phase\n",
    "        if new_loss < f_a:\n",
    "            k += 1\n",
    "            u, f_u = new_step, new_loss\n",
    "            \n",
    "            while f_b is None:\n",
    "                k += 1\n",
    "                new_step = 0.2 * 2**k * mag_grad\n",
    "                new_weights = weights - new_step * normalized_grad\n",
    "                self._set_flat_params(new_weights)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    new_loss = closure()\n",
    "                count_eval += 1\n",
    "                \n",
    "                if new_loss < f_u:\n",
    "                    a, f_a = u, f_u\n",
    "                    u, f_u = new_step, new_loss\n",
    "                    if count_eval == budget:\n",
    "                        return u, f_u\n",
    "                elif new_loss > f_u:\n",
    "                    b, f_b = new_step, new_loss\n",
    "                    if count_eval == budget:\n",
    "                        return u, f_u\n",
    "                else:\n",
    "                    u = (a + new_step) / 2\n",
    "                    new_weights = weights - u * normalized_grad\n",
    "                    self._set_flat_params(new_weights)\n",
    "                    with torch.no_grad():\n",
    "                        new_loss = closure()\n",
    "                    return u, new_loss\n",
    "                    \n",
    "        elif new_loss > f_a:\n",
    "            b, f_b = new_step, new_loss\n",
    "            \n",
    "            while f_u is None:\n",
    "                if abs(b - a) < tolerance or abs(f_a - f_b) < tolerance:\n",
    "                    return b, f_b\n",
    "                    \n",
    "                new_step = a + 0.382 * (b - a)\n",
    "                new_weights = weights - new_step * normalized_grad\n",
    "                self._set_flat_params(new_weights)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    new_loss = closure()\n",
    "                count_eval += 1\n",
    "                \n",
    "                if new_loss < f_a:\n",
    "                    u, f_u = new_step, new_loss\n",
    "                    if count_eval == budget:\n",
    "                        return u, f_u\n",
    "                elif new_loss > f_a:\n",
    "                    b, f_b = new_step, new_loss\n",
    "                    if count_eval == budget:\n",
    "                        return b, f_b\n",
    "                else:\n",
    "                    u = (a + new_step) / 2\n",
    "                    new_weights = weights - u * normalized_grad\n",
    "                    self._set_flat_params(new_weights)\n",
    "                    with torch.no_grad():\n",
    "                        new_loss = closure()\n",
    "                    return u, new_loss\n",
    "        else:\n",
    "            u = (a + new_step) / 2\n",
    "            new_weights = weights - u * normalized_grad\n",
    "            self._set_flat_params(new_weights)\n",
    "            with torch.no_grad():\n",
    "                new_loss = closure()\n",
    "            return u, new_loss\n",
    "        \n",
    "        # Golden section search phase\n",
    "        x, f_x = u, f_u\n",
    "        \n",
    "        while count_eval != budget:\n",
    "            if abs(b - a) < tolerance or abs(f_a - f_b) < tolerance:\n",
    "                return x, f_x\n",
    "                \n",
    "            d_golden = (b - a) * 0.382\n",
    "            if (b - x) > (x - a):\n",
    "                u = a + d_golden\n",
    "            else:\n",
    "                u = b - d_golden\n",
    "                \n",
    "            new_weights = weights - u * normalized_grad\n",
    "            self._set_flat_params(new_weights)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                f_u = closure()\n",
    "            count_eval += 1\n",
    "            \n",
    "            if f_u < f_x:\n",
    "                if u >= x:\n",
    "                    a, f_a = x, f_x\n",
    "                elif u < x:\n",
    "                    b, f_b = x, f_x\n",
    "                x, f_x = u, f_u\n",
    "            elif f_u > f_x:\n",
    "                if u < x:\n",
    "                    a, f_a = u, f_u\n",
    "                elif u > x:\n",
    "                    b, f_b = u, f_u\n",
    "            else:\n",
    "                return x, f_x\n",
    "                \n",
    "        return x, f_x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self, closure):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \n",
    "        Args:\n",
    "            closure (callable): A closure that reevaluates the model and returns the loss.\n",
    "                               Must call backward() to compute gradients.\n",
    "        \n",
    "        Returns:\n",
    "            loss: The loss value after the step\n",
    "        \"\"\"\n",
    "        assert closure is not None, \"L-BFGS requires a closure function\"\n",
    "        \n",
    "        # Evaluate loss and compute gradients\n",
    "        with torch.enable_grad():\n",
    "            loss = closure()\n",
    "        \n",
    "        # Get hyperparameters\n",
    "        max_history = self.param_groups[0]['max_history']\n",
    "        budget = self.param_groups[0]['line_search_budget']\n",
    "        \n",
    "        # Gather flat gradients and weights\n",
    "        flat_grad = self._gather_flat_grad()\n",
    "        flat_weights = self._gather_flat_params()\n",
    "        \n",
    "        # Store backup of current parameters\n",
    "        params_backup = self._clone_params()\n",
    "        \n",
    "        # Update s and y lists\n",
    "        if self.state['initialized']:\n",
    "            y_k = flat_grad - self.state['prev_grad']\n",
    "            s_k = self.state['s_k']\n",
    "            \n",
    "            # Maintain history size\n",
    "            if len(self.state['s_list']) == max_history:\n",
    "                self.state['s_list'].pop(0)\n",
    "                self.state['y_list'].pop(0)\n",
    "            \n",
    "            self.state['s_list'].append(s_k)\n",
    "            self.state['y_list'].append(y_k)\n",
    "        \n",
    "        # Two-loop recursion\n",
    "        q = flat_grad.clone()\n",
    "        alpha = []\n",
    "        rho_array = []\n",
    "        reset = False\n",
    "        \n",
    "        for i in range(len(self.state['s_list']) - 1, -1, -1):\n",
    "            s_i = self.state['s_list'][i]\n",
    "            y_i = self.state['y_list'][i]\n",
    "            \n",
    "            dot_product = torch.dot(y_i, s_i)\n",
    "            rho_i = 1.0 / (dot_product + 1e-6)\n",
    "            \n",
    "            # Check for numerical issues\n",
    "            mag_den = torch.norm(y_i).item() * torch.norm(s_i).item()\n",
    "            if mag_den == 0.0:\n",
    "                reset = True\n",
    "            else:\n",
    "                rho_array.append(dot_product.item() / mag_den)\n",
    "            \n",
    "            alpha_i = rho_i * torch.dot(s_i, q)\n",
    "            alpha.append(alpha_i)\n",
    "            q = q - alpha_i * y_i\n",
    "        \n",
    "        # Initial Hessian approximation\n",
    "        if len(self.state['s_list']) > 0:\n",
    "            gamma = torch.dot(self.state['s_list'][-1], self.state['y_list'][-1]) / \\\n",
    "                    torch.dot(self.state['y_list'][-1], self.state['y_list'][-1])\n",
    "        else:\n",
    "            gamma = 1.0\n",
    "        \n",
    "        r = gamma * q\n",
    "        \n",
    "        # Second loop\n",
    "        for i in range(len(self.state['s_list'])):\n",
    "            s_i = self.state['s_list'][i]\n",
    "            y_i = self.state['y_list'][i]\n",
    "            rho_i = 1.0 / (torch.dot(y_i, s_i) + 1e-6)\n",
    "            beta_i = rho_i * torch.dot(y_i, r)\n",
    "            r = r + s_i * (alpha[len(self.state['s_list']) - 1 - i] - beta_i)\n",
    "        \n",
    "        # Check for numerical issues\n",
    "        if torch.isnan(r).any() or (len(rho_array) > 0 and any(rho < 1e-4 for rho in rho_array)) or reset:\n",
    "            r = flat_grad\n",
    "            self.state['s_list'] = []\n",
    "            self.state['y_list'] = []\n",
    "            print(\"Warning: L-BFGS reset due to numerical issues\")\n",
    "        \n",
    "        # Line search using Brent's method\n",
    "        mag_r = torch.norm(r, p=2).item()\n",
    "        \n",
    "        # Create a closure that just evaluates loss (no backward)\n",
    "        def line_search_closure():\n",
    "            with torch.enable_grad():\n",
    "                loss_val = closure()\n",
    "            return loss_val.item()\n",
    "        \n",
    "        step_size, new_loss = self._brent_line_search(\n",
    "            line_search_closure, \n",
    "            flat_weights, \n",
    "            r, \n",
    "            loss.item(), \n",
    "            budget\n",
    "        )\n",
    "        \n",
    "        # Update parameters\n",
    "        new_weights = flat_weights - step_size * r / mag_r\n",
    "        self._set_flat_params(new_weights)\n",
    "        \n",
    "        # Store for next iteration\n",
    "        self.state['s_k'] = -step_size * r / mag_r\n",
    "        self.state['prev_grad'] = flat_grad.clone()\n",
    "        self.state['initialized'] = True\n",
    "        \n",
    "        return torch.tensor(new_loss)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name_array=[\"FCP\",\"LN\",\"AE\",\"LSTM\"]\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=len(trainset), shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=len(testset), shuffle=True)\n",
    "for images, labels in train_loader:\n",
    "    images_train, labels_train = images.to(device), labels.to(device)\n",
    "    break\n",
    "for images, labels in test_loader:\n",
    "    images_test, labels_test = images.to(device), labels.to(device)\n",
    "    break\n",
    "\n",
    "if model_name == \"LN\" :\n",
    "    net = LeNet().to(device)\n",
    "elif model_name == \"FCP\" :\n",
    "    net = FCP().to(device)\n",
    "elif model_name == \"AE\" :\n",
    "    net = Autoencoder().to(device)\n",
    "elif model_name == \"LSTM\" :\n",
    "    net=LSTM_sMNIST().to(device)\n",
    "if model_name == \"AE\":\n",
    "    criterion = nn.MSELoss()\n",
    "else:\n",
    "    criterion= nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = LBFGS_Brent(net.parameters(), max_history=10, line_search_budget=6)\n",
    "net.train()\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    output = net(images_train)\n",
    "    loss = criterion(output, labels_train)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "def closure_AE():\n",
    "    optimizer.zero_grad()\n",
    "    output = net(images_train)\n",
    "    loss = criterion(output, images_train)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "for epoch in range(100):\n",
    "    if model_name == \"AE\":\n",
    "        loss = optimizer.step(closure_AE)\n",
    "    else:\n",
    "        loss= optimizer.step(closure)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
